{
  "Tool Name": "Moonshot: A Testing Framework for Large Language Models",
  "Speakers": [
    "Thomas Tay",
    "Seok Min Lim",
    "Lionel Teo"
  ],
  "Tracks": [
    "Track: Vulnerability Assessment"
  ],
  "Event": "BH-ARSENAL",
  "Github URL": "https://github.com/ryanbgriffiths/IROS2023PaperList/blob/main/README.md",
  "Description": "In today's rapidly evolving AI landscape, large language models (LLMs) have emerged as a cornerstone of many AI-driven solutions, offering increasingly remarkable capabilities in use cases like chatbots and code generation.\n\nHowever, this advancement also introduces a unique set of security and safety challenges, ranging from data privacy risks, biases in model outputs, ethical implications of AI interactions, to the risks of generating and executing malicious codes when using these new AI systems. Unfortunately, current LLM testing often focuses on evaluating performance over addressing these vulnerabilities.\n\nWe present Moonshot \u2013 a testing tookit designed specifically for security evaluators, penetration testers, red teamers, and bug-bounty hunters to conduct attacks on large language models. Moonshot distinguishes itself through its extensible and modular design, facilitating the systematic creation, testing and execution of attacks on LLMs. It comes equipped with a suite of pre-defined security vulnerabilities and safety tests, while also offering users the ease of integrating their own tests into the framework. Additionally, Moonshot features a specialised red-teaming interface that drastically streamlines the process of vulnerability assessment across various LLMs for red teamers.\n\nMoonshot is designed with a simple, intuitive, and interactive interface that would be familiar to AI developers and security experts. Additionally, Moonshot is engineered for easy integration into any model development workflow, enabling seamless and repeatable testing for model developers.",
  "Year": "2024",
  "Location": "Asia"
}