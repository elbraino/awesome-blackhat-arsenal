{
  "Tool Name": "ART: Adversarial Robustness 360 Toolbox for Machine Learning Models",
  "Speakers": [
    "Irina Nicolae",
    "Beat Buesser"
  ],
  "Tracks": [
    "Track: Vulnerability Assessment"
  ],
  "Event": "BH-ARSENAL",
  "Github URL": "https://github.com/lfai/proposing-projects/blob/master/proposals/trusted-ai.adoc",
  "Description": "Adversarial attacks against machine learning systems have become an indisputable threat. Attackers can compromise the training of machine learning models by injecting malicious data into the training set (so-called poisoning attacks), or by crafting adversarial samples that exploit the blind spots of machine learning models at test time (so-called evasion attacks). These attacks have been demonstrated in a number of different application domains, including malware detection, spam filtering, visual recognition, speech-to-text conversion, and natural language understanding. Devising comprehensive defences against poisoning and evasion attacks by adaptive adversaries is still an open challenge.\n\nWe will present the Adversarial Robustness 360 Toolbox (ART), a library which allows rapid crafting and analysis of both attacks and defense methods for machine learning models. ART provides an implementation for many state-of-the-art methods for attacking and defending machine learning. At Black Hat, we will introduce the major version 1.0, which contains new powerful black-box attacks, support for additional machine learning libraries, as well as new defenses and detectors. Through ART, the attendees will (re)discover how to attack and defend diverse machine learning systems.",
  "Year": "2019",
  "Location": "Europe"
}