{
  "Tool Name": "Nova Rule: The Prompt Pattern Matching",
  "Speakers": [
    "Thomas Roccia"
  ],
  "Tracks": [
    "Track: Threat Hunting"
  ],
  "Event": "BH-ARSENAL",
  "Github URL": "https://github.com/oskardudycz/ArchitectureWeekly/blob/main/Summary.md",
  "Description": "With the widespread adoption of LLMs and Generative AI, individuals and organizations are leveraging these technologies daily, whether for customer support automation, code generation, or business automation. But with increased adoption comes new security risks. The attack surface is expanding, and security teams still lack clear strategies for detecting malicious GenAI activity.\n\nHow do you identify threat actor activity in your AI system?\nHow do you detect malicious prompt usage?\nHow can threat intelligence teams hunt for adversarial GenAI behavior?\n\nThat's where NOVA comes in.\n\nNOVA is an early-stage tool for prompt hunting, designed to detect malicious or policy-violating prompts within GenAI systems. If your organization runs an AI-powered service, you might need a way to monitor, analyze, and detect specific prompt patterns before they lead to abuse, data leaks, or security incidents.",
  "Year": "2025",
  "Location": "USA"
}