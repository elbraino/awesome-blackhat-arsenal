{
  "Tool Name": "LlamaFirewall: Guardrails for Controlling Agentic AI Systems",
  "Speakers": [
    "Cyrus Nikolaidis",
    "Kat He",
    "Stephanie Ding",
    "Sahana CB"
  ],
  "Tracks": [
    "Track: AI",
    "ML & Data Science"
  ],
  "Event": "BH-ARSENAL",
  "Github URL": null,
  "Description": "Meta's AI Security team is releasing three new models under the LlamaFirewall framework, targeting a key challenge in deploying AI agents: controllability. Controllability means making sure an AI agent, even one handling sensitive data or high-stakes actions, sticks to the rules set by its developers or users. Without proper controls, an agent could stray from its goals or even be exploited, potentially leading to data leaks or unauthorized actions.\n\nUsing fine-tuning on small, pre-trained Llama models, our guardrails are more efficient and performant than previous solutions. Packaged within the LlamaFirewall framework, these compact models deliver a lightweight yet robust security layer that ensures AI agents remain safe and aligned with their intended purpose.",
  "Year": "2025",
  "Location": "USA"
}